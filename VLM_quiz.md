# کوئیزهای دوره VLM 
---

## کوئیز 1
### سوال 1
**کدام اندازه برای بردار (embedding) خروجی Vision Encoder و Text Encoder در CLIP صحیح است؟**
* 128
* 256
* 512
* 1024

### سوال 2
**مزیت اصلی قابلیت‌های zero-shot در CLIP چیست؟** 
* به حداقل رساندن منابع محاسباتی مورد نیاز
* توانایی انجام وظایف بدون نیاز به داده‌های آموزشی اضافی مخصوص آن وظیفه
* افزایش چشمگیر سرعت آموزش
* استفاده بهینه از مجموعه داده‌های آموزشی کوچک

### سوال 3
**محدودیت اصلی CLIP برای وظایف طبقه‌بندی بصری دقیق (fine-grained) چیست؟**
* معماری dual-encoder آن سربار محاسباتی قابل توجهی را تحمیل می‌کند که مانع از استنتاج دقیق و لحظه‌ای می‌شود.
* رویکرد مدل‌سازی مولد اصلی آن، انسجام تصویر-متن را بر یادگیری ویژگی‌های متمایزکننده برای جزئیات ظریف اولویت می‌دهد.
* پیش‌آموزش بر روی داده‌های وب با برچسب‌گذاری گسترده، بازنمایی‌هایی را تولید می‌کند که اغلب فاقد دقت لازم برای تشخیص زیرمجموعه‌های بصری مشابه هستند.
* واژگان ثابت و گسترده Text Encoder برای نمایش دقیق ظرایف متنی که برای تمایزات دقیق حیاتی است، دچار مشکل می‌شود.

### سوال 4
**اگر در طول آموزش، یک تصویر "بالن هوای گرم" با متن "مردی با ماسک آبی" جفت شود، مدل به دنبال چه امتیاز شباهت کسینوسی خواهد بود؟**
* نزدیک به 1.0
* دقیقاً برابر با 0.5
* نزدیک به 0.0
* دقیقاً برابر با 0.7

### سوال 5
**در فرمول تابع ضرر $\mathcal{L}_{CLIP}$، هدف از محاسبه شباهت‌ها بین یک تصویر I و تمام متون T در یک دسته (batch) (و برعکس برای متون و تصاویر) چیست؟** 
* اطمینان از اینکه هر تصویر فقط با توضیحات خودش مقایسه می‌شود.
* نرمال‌سازی بردارها (embeddings) قبل از محاسبه ضرر.
* ایجاد توزیعی از شباهت‌ها، که به مدل امکان می‌دهد جفت‌های صحیح را از جفت‌های نادرست تشخیص دهد.
* محاسبه میانگین ویژگی‌های تمام متون برای هر تصویر.
* 
### سوال 6
**ویژگی اصلی که مدل SigLIP را از مدل‌های مشابه مانند CLIP متمایز می‌کند، کدام است؟**

* استفاده از یک معماری transformer مشترک برای هر دو حالت تصویر و متن.
* بهره‌گیری از تابع ضرر sigmoid به جای تابع ضرر softmax برای آموزش.
* کاهش ابعاد بردار (embedding) به 256 برای افزایش سرعت استنتاج.
* استفاده انحصاری از داده‌های تصویری بدون نیاز به برچسب‌های متنی برای آموزش.
* 
### سوال 7
**چه مزایایی در استفاده از تابع ضرر sigmoid در SigLIP نسبت به softmax در CLIP وجود دارد؟**

* Sigmoid در تشخیص بین جفت‌های مثبت و منفی کمتر کارآمد است.
* Sigmoid به دلیل وابستگی به سایر جفت‌ها در دسته، کندتر عمل می‌کند.
* Sigmoid به دلیل بهینه‌سازی کلی، به حافظه بیشتری نیاز دارد.
* Sigmoid عملکرد بهتری را در اندازه‌های دسته (batch size) کوچک‌تر نشان می‌دهد.

---

## کوئیز 2
### سوال 1
**در قطعه کد `inputs = tokenizer(text, padding=True, return_tensors="pt")`، آرگومان `padding=True` چه چیزی را تضمین می‌کند؟**
* افزودن توکن‌های مخصوص شروع و پایان توالی به هر رشته ورودی در دسته، استانداردسازی فرمت آن‌ها برای مدل.
* کوتاه کردن تمام توالی‌های ورودی به یک طول حداکثر از پیش تعریف شده در صورت تجاوز از آن، جلوگیری از خطای ورودی‌های بیش از حد طولانی.
* یکسان‌سازی طول تمام توالی‌های توکن‌سازی شده در دسته با افزودن توکن‌های padding به توالی‌های کوتاه‌تر، که پردازش دسته‌ای توسط مدل را تسهیل می‌کند.
* تنظیم پویای واژگان توکنایزر بر اساس دسته ورودی، جایگزینی کلمات خارج از واژگان با توکن‌های padding برای حفظ شناسه‌های توکن ثابت.

### سوال 2
**CLIP هر دو تصویر و متن را به یک "فضای برداری مشترک" نگاشت می‌کند.مزیت اصلی داشتن هر دو حالت در یک فضای برداری برای وظایفی مانند مقایسه یک تصویر با یک متن چیست؟** 
* فضای ذخیره‌سازی کلی مورد نیاز برای مدل را کاهش می‌دهد.
* امکان مقایسه مستقیم عددی بردارهای تصویر و متن را برای سنجش ارتباط معنایی آن‌ها فراهم می‌کند.
* باعث می‌شود که Image Encoder و Text Encoder در معماری یکسان باشند.
* به مدل اجازه می‌دهد تا با یادگیری ضمنی یک "فضای مفهومی" میانی که دو حالت را به هم متصل می‌کند، انتقال "zero-shot" را انجام دهد.

### سوال 3
**شکل تنسور `input_ids` برای متن ورودی `["a donut", "a cookie", "an airplane", "a cat"]` پس از توکن‌سازی شامل توکن‌های ویژه با `padding=True` چیست؟** 
* `torch.Size([4, 512])`
* `torch.Size([4, 4])`
* `torch.Size([4, 1])`
* `torch.Size([1, 4])`

### سوال 4
**فراتر از بررسی شباهت ساده بین چند آیتم، بردارهای یاد گرفته شده (embeddings) متن و تصاویر چگونه در کاربردهای پیشرفته‌تر یادگیری ماشین مفید هستند؟** 
* در درجه اول برای تجسم شباهت‌های زوجی از طریق نقشه‌های حرارتی و نمودارهای مشابه برای درک توزیع داده.
* عمدتاً برای بهبود کارایی توکنایزرها و پردازشگرهای تصویر با جاسازی مستقیم این اجزا در آموزش مدل.
* به عنوان نمایش ویژگی‌های با کیفیت بالا که وظایفی مانند بازیابی تصویر، طبقه‌بندی zero-shot بدون داده‌های خاص وظیفه، و وظایف تولید چندوجهی را ممکن می‌سازد.
* اساساً برای فشرده‌سازی داده‌های با ابعاد بالا صرفاً برای سرعت بخشیدن به استنتاج مدل با کاهش پیچیدگی ابعادی.

### سوال 5
**برای وظایف متقابل-حالتی مانند بازیابی متن به تصویر، کدام ویژگی فراتر از شباهت بالا برای جفت‌های منطبق، حیاتی است، و فرایند آموزش مدل‌هایی مانند CLIP چگونه این را تشویق می‌کند؟** 
* بردارهای (embeddings) برای جفت‌های منطبق باید بیت به بیت یکسان باشند؛ این امر با یک هدف آموزشی مولد که یک حالت را از حالت دیگر بازسازی می‌کند، به دست می‌آید.
* اندازه (طول) بردارهای برای تصاویر باید به طور مداوم بزرگتر از متن باشد؛ این با لایه‌های نرمال‌سازی جداگانه برای هر کدگذار حالت اعمال می‌شود.
* بردارهای برای جفت‌های تصویر-متن نامشابه باید در فضای برداری از هم دور شوند؛ این معمولاً از طریق یک تابع ضرر کنتراستیو (contrastive loss) به دست می‌آید.
* Text Encoder باید بردارهایی تولید کند که تبدیل‌های خطی ساده از بردارهای Image Encoder باشند؛ این یک نگاشت مستقیم و قابل تفسیر بین حالت‌ها را تضمین می‌کند.

---

## کوئیز 3
### سوال 1
**یک برچسب متنی به 6 توکن (شامل شروع/پایان) توکن‌سازی می‌شود. اگر دسته به حداکثر طول 7 پر شود، احتمالاً attention mask برای این برچسب چه خواهد بود و چرا؟**
* `[1,1,1,1,1,1,1]` - تمام توکن‌ها به طور یکسان مورد توجه قرار می‌گیرند.
* `[1,1,1,1,1,1,0]` - توکن‌های padding را نادیده می‌گیرد.
* `[0,0,0,0,0,0,1]` - فقط توکن padding نهایی مهم است.
* ماسک برای توالی‌های طولانی‌تر حذف می‌شود.

### سوال 2
**اگر شباهت کسینوسی خام 0.25 و دما (`logit_scale.exp()`) برابر 100 باشد، امتیاز `logit_per_image` قبل از softmax چقدر است و چرا از مقیاس‌بندی دما در CLIP استفاده می‌شود؟** 
* `score=0.0025`, مقیاس‌بندی جدایی امتیاز را کاهش می‌دهد، اعتماد را پایین می‌آورد.
* `score = 25`, مقیاس‌بندی توزیع شباهت را تیزتر می‌کند و به تشخیص دقیق‌تر در طول آموزش کمک می‌کند.
* `score = 25`, مقیاس‌بندی اعمال می‌شود تا اطمینان حاصل شود که تمام مقادیر logit قبل از softmax مثبت هستند.
* `score = 0.25`, نرخ یادگیری Text Encoder را کنترل می‌کند.

### سوال 3
**تنسور `pixel_values` (با شکل `[1, 3, 224, 224]`) حاوی مقادیر منفی است. با توجه به اینکه مقادیر پیکسل تصویر خام معمولاً در محدوده [0, 255] یا [0, 1] هستند، چرا این `pixel_values` پردازش شده اعداد منفی را نشان می‌دهند؟** 
* مقادیر منفی یک مصنوع از تبدیل `return_tensors="pt"` هستند، جایی که PyTorch به طور پیش‌فرض از یک نوع عدد صحیح علامت‌دار برای داده‌های پیکسل استفاده می‌کند.
* `CLIPImageProcessor` نوع خاصی از تبدیل فضای رنگی را اعمال می‌کند که در آن کانال‌ها به طور طبیعی می‌توانند مقادیر منفی داشته باشند.
* پردازشگر CLIP نرمال‌سازی را اعمال می‌کند. اگر مقدار اصلی یک پیکسل کمتر از میانگین کانال باشد، مقدار نرمال‌شده منفی خواهد بود.
* مقادیر منفی در `pixel_values` به طور عمدی توسط پردازشگر معرفی می‌شوند تا نشان دهند پچ‌های تصویری که باید توجه کمتری دریافت کنند یا توسط Vision Transformer در طول استخراج ویژگی ماسک شوند.

### سوال 4
**در نوت‌بوک نشان داده شده است که CLIP یک تصویر را در برابر چند برچسب متنی طبقه‌بندی می‌کند. چگونه این می‌تواند به بازیابی تصویر با واژگان باز (open-vocabulary) و در مقیاس بزرگ گسترش یابد، و چه مراحل اضافی مورد نیاز است؟**
* امکان‌پذیر نیست؛ `logits_per_image` محدود به چند برچسب متنی ثابت است.
* CLIP را روی پایگاه داده تصویر با تمام پرس‌وجوهای ممکن تنظیم (fine-tune) کنید و آن را تحت نظارت قرار دهید.
* بردارها (embeddings) تصویر را برای پایگاه داده از قبل محاسبه کنید. برای یک پرس‌وجو، متن را جاسازی کرده و بردارهای تصویر ذخیره شده را برای شباهت کسینوسی بالا جستجو کنید.
* یک طبقه‌بندی‌کننده منحصر به فرد برای هر پرس‌وجوی متنی بالقوه با استفاده از ویژگی‌های تصویر CLIP آموزش دهید، سپس تمام این طبقه‌بندی‌کننده‌ها را بر روی هر تصویر پایگاه داده برای بازیابی اجرا کنید.

### سوال 5
**اگر `inputs.attention_mask` برای پردازش متن به اشتباه برای یک برچسب متنی معین، حتی اگر حاوی توکن‌های معتبر باشد، به همه صفر تنظیم شود، پیامد احتمالی برای `text_embeds` تولید شده برای آن برچسب و به دنبال آن برای `logits_per_image` هنگام مقایسه با یک تصویر چه خواهد بود؟** 
* هیچ تاثیری نخواهد داشت، زیرا Text Transformer می‌تواند طول محتوای واقعی را از توکن‌های ویژه پایان توالی استنتاج کند.
* `text_embeds` به احتمال زیاد یک "توالی خالی" بی‌معنی را نشان می‌دهد که منجر به امتیازات `logits_per_image` بسیار پایین می‌شود.
* باعث خطای زمان اجرا می‌شود زیرا مدل نمی‌تواند توالی‌ها را با یک ماسک توجه تماماً صفر پردازش کند.
* `text_embeds` به طور غیرمعمولی قوی و بیش از حد مطمئن خواهد بود، زیرا مدل سعی می‌کند از آنچه به عنوان توالی‌ای متشکل از توکن‌های padding بسیار مهم (mask-in) درک می‌کند، معنا استخراج کند.

---

## کوئیز 4
### سوال 1
**چرا از نقش "system" هنگام تعامل با مدل Qwen 2.5VL از طریق چت استفاده می‌شود؟**
* برای ارائه دستورات متنی موقتی که فقط بر پاسخ فوری بعدی مدل تأثیر می‌گذارد.
* برای تعریف یا محدود کردن کلی رفتار مکالمه و فرمت پاسخ.
* برای شروع اصلاح مستقیم وزن‌ها یا پارامترهای پیکربندی مدل در طول جلسات استنتاج.
* برای درج مستقیم داده‌های تصویر و ویدیو در جریان مکالمه، مستقل از دستورالعمل‌های مبتنی بر متن.

### سوال 2
**پارامتر `messages` که به `client.chat.completions.create()` ارسال می‌شود، به طور مداوم چه چیزی را نشان می‌دهد؟**
* یک شیء دیکشنری واحد که آخرین دور را نشان می‌دهد.
* یک رشته JSON که حاوی کل تاریخچه مکالمه است.
* یک آرایه از اشیاء دیکشنری، که در آن هر دیکشنری یک دور با یک نقش (role) و محتوا (content) را نشان می‌دهد.
* یک دیکشنری تو در تو که کلیدها، برچسب‌های زمانی و مقادیر، اشیاء پیام هستند.

### سوال 3
**مرجع تصویر در آرایه محتوا با کدام ساختار کلید-مقدار تعریف می‌شود؟**
* `type: "image_url", data: {"image_src": "..."}`
* `type: "image_url", image_url: {"url": "..."}`
* `content_type: "image", image: {"source_url": "..."}`
* `type: "url", image_content: {"url_path": "..."}`

### سوال 4
**مدل Qwen 2.5-VL-7B، که در دقت 8-بیت اجرا می‌شود، با کدام یک از موارد زیر تنگ‌ترین توصیه VRAM را دارد؟** 
* RTX 3060 12GB
* RTX 4060 8GB
* RTX 4070 Ti 16GB
* RTX 4090 24GB

### سوال 5
**شرایط مجوز مدل Qwen 2.5-VL-3B-Instruct، همانطور که در اسلایدها ذکر شده است، چیست؟** 
* متن‌باز Apache 2.0 برای استفاده تجاری و تحقیقاتی.
* استفاده تجاری مجاز، استفاده تحقیقاتی ممنوع است.
* مالکیت با محدودیت‌ها، به ویژه اجازه استفاده غیرتجاری یا تحقیقاتی.
* استفاده، اصلاح و توزیع مجدد کاملاً نامحدود.

---

## کوئیز 5
### سوال 1
**هدف از `process_vision_info(msgs)` چیست؟** 
* تبدیل دستورات متنی به نمایش‌های بصری.
* نمایش تصاویر و ویدیوها در نوت‌بوک.
* استخراج و پیش‌پردازش داده‌های تصویر/ویدیو از لیست `msgs`.
* تولید زیرنویس‌های جایگزین برای محتوای بصری.

### سوال 2
**چه چیزی نسخه "instruct" مدل‌های Qwen-VL را از نسخه‌های غیر-instruct متمایز می‌کند؟** 
* منحصراً از روش‌های پیش‌آموزش بدون نظارت برای تعمیم‌پذیری بهتر استفاده می‌کند.
* لایه‌های توجه با دقت بالا را به طور خاص برای مدیریت تصاویر با وضوح بالا بهینه می‌کند.
* شامل آموزش تراز دستورالعمل-پیروی اضافی است که پاسخ‌ها به دستورات صریح کاربر را بهبود می‌بخشد.
* به جای فضاهای جاسازی مشترک، از کدگذارهای جداگانه مخصوص هر حالت استفاده می‌کند.

### سوال 3
**اگر زیرنویس‌های مدل Qwen-VL بیش از حد کلی به نظر می‌رسند، چه استراتژی‌ای می‌تواند به طور موثر دقت زیرنویس را افزایش دهد؟** 
* افزایش اندازه دسته (batch size) در طول استنتاج.
* تنظیم دقیق مدل بر روی مجموعه‌داده‌های خاص دامنه.
* استفاده از تصاویر با وضوح پایین‌تر به عنوان ورودی.
* منحصراً تغییر استراتژی‌های توکن‌سازی بدون آموزش بیشتر.

### سوال 4
**هنگامی که تابع `processor()` برای ایجاد ورودی‌های مدل فراخوانی می‌شود، چه چیزی به پارامتر `images` آن ارسال می‌شود؟** 
* شیء خام `PIL.Image`
* رشته `text_prompt`
* تنسور `image_inputs` که از `process_vision_info` به دست آمده است.
* یک لیست از URLها که به تصاویر اشاره دارند.

### سوال 5
**چرا استفاده از `torch_dtype='auto'` ممکن است سرعت استنتاج را در GPU بهبود بخشد؟**
* اندازه مدل را به صورت پویا در طول استنتاج مقیاس‌بندی می‌کند.
* قالب‌های دقت کاهش یافته را انتخاب می‌کند و توان محاسباتی را افزایش می‌دهد.
* پردازش دسته‌ای ناهمگام را به طور پیش‌فرض فعال می‌کند.
* مستقیماً سرهای توجه را بر اساس حافظه GPU موجود بهینه می‌کند.
